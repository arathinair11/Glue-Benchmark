# Glue-Benchmark
Natural Language Understanding (NLU) models are generally designed for a specific task and struggle to perform efficiently with out-of-domain data. It is critical to develop a unified model which can be accepted universally and process language from any dataset. In this project report, we will be applying various natural language processing methods and machine learning models on multiple datasets in Generalized Language Understanding Evaluation (GLUE) benchmark. The GLUE Benchmark provides model diagnostic test suite that enables detailed linguistic analysis of NLU models. The key objective of this project is to train, analyze and evaluate the ability of our proposed NLP model in making accurate predictions across the diverse range of NLU tasks specified by GLUE benchmark.  

To create a high-performing model capable of understanding and generating natural language, we integrate three state-of-the-art models -- Electra, XLNet, and DeBERTa -- together with an ensemble learning model to capture information from the three disparate models on each task. With this method, we capitalize on the unique strengths of each individual model, while mitigating most weaknesses.
